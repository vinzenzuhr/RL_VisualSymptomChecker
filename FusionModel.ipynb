{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "#from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Clean up\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, folder_path, disease_list):\n",
    "        self.folder_path = folder_path\n",
    "        self.disease_list = disease_list\n",
    "        self.folders_with_diseases_labels = {}\n",
    "        self.folder_name_with_diseases = []\n",
    "        self.label_counts = None\n",
    "        self.data = []\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        imgs_folder = os.path.join(self.folder_path, \"imgs\")\n",
    "        subdirectories = [d for d in os.listdir(imgs_folder) if os.path.isdir(os.path.join(imgs_folder, d))]\n",
    "\n",
    "        for folder in subdirectories:\n",
    "            # detection file: the report of images\n",
    "            detection_file_path = os.path.join(imgs_folder, folder, 'detection.json')\n",
    "            with open(detection_file_path, 'r') as detection_file:\n",
    "                detection_data = json.load(detection_file)\n",
    "\n",
    "                disease_labels = [label for item in detection_data for label in item.keys() if label in self.disease_list]\n",
    "\n",
    "                # merge labels for images with multiple labels\n",
    "                if disease_labels and len(disease_labels)==1:\n",
    "                    self.folders_with_diseases_labels[folder] = disease_labels[0]\n",
    "                    self.folder_name_with_diseases.append(folder)\n",
    "            \n",
    "                    # question file\n",
    "                    detection_file_path = os.path.join(imgs_folder, folder, \"question.json\")\n",
    "                    if os.path.exists(detection_file_path):\n",
    "                        with open(detection_file_path, \"r\") as detection_file:\n",
    "                            detection_data = json.load(detection_file)\n",
    "                            img_name = detection_data[0].get(\"img_name\", \"\")\n",
    "                            question_data = [item for item in detection_data if \"question\" in item and item.get(\"q_lang\", \"\") == \"en\"]\n",
    "                            \n",
    "                            # Concatenate all questions and answers into a single string\n",
    "                            # remove questions including disease\n",
    "                            all_qa = \" \".join(f\"Q: {item['question']} A: {item['answer']}\" for item in question_data if \"What disease\" not in item['question'])\n",
    "                            \n",
    "                            self.data.append({\n",
    "                                \"image_path\": os.path.join(imgs_folder, img_name),\n",
    "                                \"text\": all_qa, \n",
    "                                \"label\": disease_labels[0]\n",
    "                            })\n",
    "        \n",
    "       \n",
    "    def delete_folders(self):\n",
    "        # frequency of each merged label\n",
    "        self.label_counts = Counter(self.folders_with_diseases_labels.values())\n",
    "\n",
    "        # delete folders with label counts <= 3\n",
    "        folders_to_delete = [folder_name for folder_name, label in self.folders_with_diseases_labels.items() if self.label_counts[label] <= 3]\n",
    "        full_paths = [f\"Slake1.0/imgs/{folder}/source.jpg\" for folder in folders_to_delete]\n",
    "        \n",
    "        self.data = [item for item in self.data if item[\"image_path\"] not in full_paths]\n",
    "        \n",
    "        for folder_name in folders_to_delete:\n",
    "            path = 'Slake1.0/imgs'\n",
    "            \n",
    "            del self.folders_with_diseases_labels[folder_name]\n",
    "            self.folder_name_with_diseases.remove(folder_name)\n",
    "            \n",
    "            \n",
    "    def get_training_data(self):\n",
    "        # Convert labels to indices\n",
    "        label_to_index = {label: idx for idx, label in enumerate(set(self.folders_with_diseases_labels.values()))}\n",
    "        self.data = [{\"image_path\": item[\"image_path\"], \"text\": item[\"text\"], \"label\": label_to_index[item[\"label\"]]} for item in self.data]\n",
    "    \n",
    "        # 80% for training and 20% for validation\n",
    "        train_data, val_data = train_test_split(self.data, test_size=0.2, random_state=42, shuffle=True)\n",
    "        return train_data, val_data\n",
    "\n",
    "\n",
    "folder_path = 'Slake1.0'\n",
    "disease_list = ['Pneumothorax', 'Pneumonia', 'Effusion','Cardiomegaly','Lung Cancer']\n",
    "data_processor = DataProcessor(folder_path, disease_list)\n",
    "data_processor.load_data()\n",
    "#data_processor.delete_folders()\n",
    "train_data, val_data = data_processor.get_training_data()\n",
    "#training_data = data_processor.get_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of texts \n",
    "def get_tokens(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    length = len(tokens)\n",
    "    if length > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    return tokens, length  \n",
    "\n",
    "def get_masks(text, tokenizer, max_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    tokens, length = get_tokens(text, tokenizer)\n",
    "    return np.asarray([1]*len(tokens) + [0] * (max_length - len(tokens)))\n",
    "\n",
    "\n",
    "def get_segments(text, tokenizer, max_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    tokens, length = get_tokens(text, tokenizer)\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return np.asarray(segments + [0] * (max_length - len(tokens)))\n",
    "\n",
    "def get_ids(text, tokenizer, max_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    tokens, length = get_tokens(text, tokenizer)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = np.asarray(token_ids + [0] * (max_length-length))\n",
    "    return input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_processor, tokenizer, max_length, transform=None, is_train=True):\n",
    "        self.data_processor = data_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_processor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_processor[idx][\"image_path\"]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.is_train:\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = image.rotate(angle)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = self.data_processor[idx][\"text\"]\n",
    "        label = self.data_processor[idx][\"label\"]\n",
    "\n",
    "        # Preprocess text using provided functions\n",
    "        tokens, length = get_tokens(text, self.tokenizer)\n",
    "        masks = get_masks(text, self.tokenizer, self.max_length)\n",
    "        segments = get_segments(text, self.tokenizer, self.max_length)\n",
    "        ids = get_ids(text, self.tokenizer, self.max_length)\n",
    "\n",
    "        # numpy arrays to PyTorch tensors\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "        masks = torch.tensor(masks, dtype=torch.long)\n",
    "        segments = torch.tensor(segments, dtype=torch.long)\n",
    "\n",
    "        return {\"input_word_ids\": ids, \"input_mask\": masks, \"segment_ids\": segments, \"image\": image, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_length, transform=transform, is_train=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_data, tokenizer, max_length, transform=transform, is_train=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images Model with AlexNet\n",
    "class ImageModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape=(3, 224, 224)):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.alexnet = models.alexnet(pretrained=True)\n",
    "        \n",
    "        self.alexnet.classifier = nn.Sequential(*list(self.alexnet.classifier.children())[:-2])\n",
    "        \n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(256 * 8 * 8, 128)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alexnet.features(x)\n",
    "        x = self.avg_pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# text model: Bert + LSTM \n",
    "class BertLSTMModel(nn.Module):\n",
    "    def __init__(self, bert_model, max_length, lstm_hidden_size):\n",
    "        super(BertLSTMModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=lstm_hidden_size, batch_first=True)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # [CLS] token embedding for LSTM\n",
    "        lstm_input = outputs.last_hidden_state[:, 0, :]  \n",
    "        # to add time dimension for LSTM\n",
    "        lstm_output, _ = self.lstm(lstm_input.unsqueeze(1))  \n",
    "        return lstm_output.squeeze()\n",
    "\n",
    "# Fusion Model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, num_classes, lstm_hidden_size=128, dropout_rate=0.4):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.text_model = BertLSTMModel(\"bert-base-uncased\", max_length, lstm_hidden_size)\n",
    "        self.image_model = ImageModel(num_classes)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, image_input):\n",
    "        text_output = self.text_model(input_ids, attention_mask, token_type_ids)\n",
    "        image_output = self.image_model(image_input)\n",
    "        fused_input = torch.cat([text_output, image_output], dim=1)\n",
    "        x = self.fc1(fused_input)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_length = 128\n",
    "lstm_hidden_size = 128\n",
    "num_classes = 5\n",
    "batch_size = 4\n",
    "epochs = 42\n",
    "\n",
    "model = FusionModel(num_classes, lstm_hidden_size)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay = 5 * 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = \"fusion_model.pth\"\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        \n",
    "        input_ids = batch[\"input_word_ids\"].to(device)\n",
    "        attention_mask = batch[\"input_mask\"].to(device)\n",
    "        token_type_ids = batch[\"segment_ids\"].to(device)\n",
    "        image_input = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids, image_input)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct_val_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "            \n",
    "            val_input_ids = val_batch[\"input_word_ids\"].to(device)\n",
    "            val_attention_mask = val_batch[\"input_mask\"].to(device)\n",
    "            val_token_type_ids = val_batch[\"segment_ids\"].to(device)\n",
    "            val_image_input = val_batch[\"image\"].to(device)\n",
    "            val_labels = val_batch[\"label\"].to(device)\n",
    "\n",
    "            val_outputs = model(val_input_ids, val_attention_mask, val_token_type_ids, val_image_input)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            correct_val_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_predictions / len(val_dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # save the model best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# to save files\n",
    "save_path = '/storage/homefs/zh21i037/'\n",
    "filename = 'losses and accuracies.png'\n",
    "save_filename = os.path.join(save_path, filename)\n",
    "\n",
    "plt.savefig(save_filename)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
