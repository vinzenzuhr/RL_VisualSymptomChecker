{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, folder_path, disease_list):\n",
    "        self.folder_path = folder_path\n",
    "        self.disease_list = disease_list\n",
    "        self.folders_with_diseases_labels = {}\n",
    "        self.folder_name_with_diseases = []\n",
    "        self.label_counts = None\n",
    "\n",
    "    def read_data(self):\n",
    "        for root, dirs, files in os.walk(os.path.join(self.folder_path, 'imgs')):\n",
    "            for folder_name in dirs:\n",
    "                folder_path = os.path.join(root, folder_name)\n",
    "                \n",
    "                detection_file_path = os.path.join(folder_path, 'detection.json')\n",
    "                with open(detection_file_path, 'r') as detection_file:\n",
    "                    detection_data = json.load(detection_file)\n",
    "\n",
    "                    disease_labels = [label for item in detection_data for label in item.keys() if label in self.disease_list]\n",
    "\n",
    "                    # merge labels for images with multiple labels\n",
    "                    if disease_labels:\n",
    "                        merged_label = '-'.join(sorted(set(disease_labels)))\n",
    "                        self.folders_with_diseases_labels[folder_name] = merged_label\n",
    "                        self.folder_name_with_diseases.append(folder_name)\n",
    "\n",
    "    def delete_folders(self):\n",
    "        # frequency of each merged label\n",
    "        self.label_counts = Counter(self.folders_with_diseases_labels.values())\n",
    "\n",
    "        # delete folders with label counts <= 3\n",
    "        folders_to_delete = [folder_name for folder_name, label in self.folders_with_diseases_labels.items() if self.label_counts[label] <= 3]\n",
    "\n",
    "        for folder_name in folders_to_delete:\n",
    "            del self.folders_with_diseases_labels[folder_name]\n",
    "            self.folder_name_with_diseases.remove(folder_name)\n",
    "            \n",
    "    def get_training_data(self):\n",
    "        training_data = []\n",
    "        for folder_name, label in self.folders_with_diseases_labels.items():\n",
    "            folder_path = os.path.join(self.folder_path, 'imgs', folder_name)\n",
    "            image_path = os.path.join(folder_path, 'source.jpg')\n",
    "            training_data.append((image_path, label))\n",
    "        return training_data\n",
    "\n",
    "\n",
    "folder_path = 'Slake1.0'\n",
    "disease_list = ['Mass', 'Pneumothorax', 'Brain Enhancing Tumor', 'Pneumonia', 'Nodule', 'Atelectasis',\n",
    "               'Liver Cancer', 'Effusion','Cardiomegaly', 'Brain Edema', 'Lung Cancer', 'Infiltrate', \n",
    "                'Brain Non-enhancing Tumor']\n",
    "data_processor = DataProcessor(folder_path, disease_list)\n",
    "data_processor.read_data()\n",
    "data_processor.delete_folders()\n",
    "#training_data = data_processor.get_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_processor, folder_names, transform=None, is_train=True):\n",
    "        self.data_processor = data_processor\n",
    "        self.folder_names = folder_names\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # map labels to index\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(set(data_processor.folders_with_diseases_labels.values()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = self.folder_names[idx]\n",
    "        folder_path = os.path.join(self.data_processor.folder_path, 'imgs', folder_name)\n",
    "        \n",
    "        # read images 'source.jpg' in each folder\n",
    "        image_path = os.path.join(folder_path, 'source.jpg')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.is_train:\n",
    "            # Random horizontal flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "            # Random vertical flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "            # Random rotation (up to 30 degrees)\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = image.rotate(angle)\n",
    "\n",
    "\n",
    "            # Random crop\n",
    "            #i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(256, 256))\n",
    "            #image = transforms.functional.crop(image, i, j, h, w)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.data_processor.folders_with_diseases_labels[folder_name]\n",
    "        label = self.label_to_index[label]\n",
    "\n",
    "        # converting images to tensor\n",
    "        if not torch.is_tensor(image):\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 70% training and 30% validation sets\n",
    "training_data, validation_data = train_test_split(data_processor.folder_name_with_diseases, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# CustomDataset for both training and validation\n",
    "train_dataset = CustomDataset(data_processor, folder_names=training_data, transform=transform, is_train=True)\n",
    "val_dataset = CustomDataset(data_processor, folder_names=validation_data, transform=transform, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.4, l2_regularization=1e-3):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # reduce spatial dimensions\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        #classification part\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes=14\n",
    "model = ImageClassificationModel(num_classes=num_classes).to(device)\n",
    "\n",
    "label_counts = {\n",
    "    'Atelectasis': 25,\n",
    "    'Brain Edema-Brain Enhancing Tumor-Brain Non-enhancing Tumor': 73,\n",
    "    'Nodule': 26,\n",
    "    'Cardiomegaly': 33,\n",
    "    'Mass': 20,\n",
    "    'Pneumonia': 28,\n",
    "    'Brain Edema-Brain Non-enhancing Tumor': 21,\n",
    "    'Pneumothorax': 14,\n",
    "    'Lung Cancer': 18,\n",
    "    'Effusion': 12,\n",
    "    'Brain Edema': 22,\n",
    "    'Brain Edema-Brain Enhancing Tumor': 12,\n",
    "    'Infiltrate': 9,\n",
    "    'Liver Cancer': 30\n",
    "    }\n",
    "\n",
    "# inverse class frequencies\n",
    "total_samples = sum(label_counts.values())\n",
    "class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}\n",
    "\n",
    "# to normalize the weights\n",
    "total_weights = sum(class_weights.values())\n",
    "class_weights = {label: weight / total_weights for label, weight in class_weights.items()}\n",
    "\n",
    "# converting to tensor\n",
    "class_weights_tensor = torch.tensor(list(class_weights.values())).to(device)\n",
    "\n",
    "# weighted CrossEntropyLoss to handle imbalance classes\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "weight_decay = 5*1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return accuracy_score(targets.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion)\n",
    "    #scheduler.step()\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# to save files\n",
    "save_path = '/storage/homefs/zh21i037/'\n",
    "filename = 'losses and accuracies.png'\n",
    "save_filename = os.path.join(save_path, filename)\n",
    "\n",
    "plt.savefig(save_filename)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
