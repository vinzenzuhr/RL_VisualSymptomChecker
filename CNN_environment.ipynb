{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, WeightedRandomSampler\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, folder_path, disease_list):\n",
    "        self.folder_path = folder_path\n",
    "        self.disease_list = disease_list\n",
    "        self.folders_with_diseases_labels = {}\n",
    "        self.folder_name_with_diseases = []\n",
    "        self.label_counts = None\n",
    "\n",
    "    def read_data(self):\n",
    "        for root, dirs, files in os.walk(os.path.join(self.folder_path, 'imgs')):\n",
    "            for folder_name in dirs:\n",
    "                folder_path = os.path.join(root, folder_name)\n",
    "                \n",
    "                detection_file_path = os.path.join(folder_path, 'detection.json')\n",
    "                with open(detection_file_path, 'r') as detection_file:\n",
    "                    detection_data = json.load(detection_file)\n",
    "\n",
    "                    disease_labels = [label for item in detection_data for label in item.keys() if label in self.disease_list]\n",
    "                    \n",
    "                    # if disease_labels is not empty and also to remove 3 images with count = 1 labels\n",
    "                    if disease_labels and len(disease_labels)==1:\n",
    "                        \n",
    "                        self.folders_with_diseases_labels[folder_name] = disease_labels[0]\n",
    "                        self.folder_name_with_diseases.append(folder_name)\n",
    "                         \n",
    "\n",
    "    def delete_folders(self):\n",
    "        # frequency of each merged label\n",
    "        self.label_counts = Counter(self.folders_with_diseases_labels.values())\n",
    "\n",
    "        # delete folders with label counts <= 3\n",
    "        folders_to_delete = [folder_name for folder_name, label in self.folders_with_diseases_labels.items() if self.label_counts[label] <= 3]\n",
    "\n",
    "        for folder_name in folders_to_delete:\n",
    "            del self.folders_with_diseases_labels[folder_name]\n",
    "            self.folder_name_with_diseases.remove(folder_name)\n",
    "            \n",
    "    def get_training_data(self):\n",
    "        training_data = []\n",
    "        for folder_name, label in self.folders_with_diseases_labels.items():\n",
    "            folder_path = os.path.join(self.folder_path, 'imgs', folder_name)\n",
    "            image_path = os.path.join(folder_path, 'source.jpg')\n",
    "            training_data.append((image_path, label))\n",
    "        return training_data\n",
    "    \n",
    "\n",
    "\n",
    "folder_path = 'Slake1.0'\n",
    "# we use 5 diseases related to chest images (heart and lungs)\n",
    "disease_list = ['Pneumothorax', 'Pneumonia','Effusion','Cardiomegaly','Lung Cancer']\n",
    "data_processor = DataProcessor(folder_path, disease_list)\n",
    "data_processor.read_data()\n",
    "#data_processor.delete_folders()\n",
    "#data = data_processor.get_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_processor, folder_names, transform=None, is_train=True):\n",
    "        self.data_processor = data_processor\n",
    "        self.folder_names = folder_names\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # map labels to index\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(set(data_processor.folders_with_diseases_labels.values()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = self.folder_names[idx]\n",
    "        folder_path = os.path.join(self.data_processor.folder_path, 'imgs', folder_name)\n",
    "        \n",
    "        # read images 'source.jpg' in each folder\n",
    "        image_path = os.path.join(folder_path, 'source.jpg')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.is_train:\n",
    "            # Random rotation\n",
    "            angle = np.random.uniform(-10, 10)\n",
    "            image = image.rotate(angle)\n",
    "\n",
    "            # Random x shift\n",
    "            #if np.random.rand() > 0.5:\n",
    "            shift_x = np.random.uniform(-10, 10)\n",
    "            image = transforms.functional.affine(image, angle=0, translate=(shift_x, 0), scale=1, shear=0)\n",
    "\n",
    "            # Random y shift\n",
    "            #if np.random.rand() > 0.5:\n",
    "            shift_y = np.random.uniform(-10, 10)\n",
    "            image = transforms.functional.affine(image, angle=0, translate=(0, shift_y), scale=1, shear=0)\n",
    "\n",
    "            # Random crop \n",
    "            if np.random.rand() > 0.5:\n",
    "                i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(256, 256))\n",
    "                image = transforms.functional.crop(image, i, j, h, w)\n",
    "                \n",
    "            if np.random.rand() > 0.5:\n",
    "                contrast_factor = np.random.uniform(0.5, 0.8)\n",
    "                image = transforms.functional.adjust_contrast(image, contrast_factor)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.data_processor.folders_with_diseases_labels[folder_name]\n",
    "        label = self.label_to_index[label]\n",
    "\n",
    "        if not torch.is_tensor(image):\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def is_minority_class(self, label_index):\n",
    "        MINORITY_THRESHOLD = 20\n",
    "        return self.class_counts[label_index] < MINORITY_THRESHOLD\n",
    "    \n",
    "    def print_class_mapping(self):\n",
    "        print(\"Class Index to Label Mapping:\")\n",
    "        for label, index in self.label_to_index.items():\n",
    "            print(f\"Index {index}: {label}\")\n",
    "            \n",
    "    def print_class_samples(self):\n",
    "        augmented_counts = Counter()\n",
    "\n",
    "        for idx in range(len(self.folder_names)):\n",
    "            folder_name = self.folder_names[idx]\n",
    "            label = self.data_processor.folders_with_diseases_labels[folder_name]\n",
    "            label_index = self.label_to_index[label]\n",
    "\n",
    "            if self.is_train and self.is_minority_class(label_index):\n",
    "                augmented_counts[label_index] += 1\n",
    "\n",
    "        print(\"Number of samples after data augmentation for each class:\")\n",
    "        for index, count in augmented_counts.items():\n",
    "            label = [label for label, idx in self.label_to_index.items() if idx == index][0]\n",
    "            print(f\"Class '{label}': {count} samples\")\n",
    "            \n",
    "    def print_class_labels_count(self):\n",
    "        label_counts = Counter()\n",
    "\n",
    "        for idx in range(len(self.folder_names)):\n",
    "            folder_name = self.folder_names[idx]\n",
    "            label = self.data_processor.folders_with_diseases_labels[folder_name]\n",
    "            label_index = self.label_to_index[label]\n",
    "            label_counts[label_index] += 1\n",
    "\n",
    "        print(\"Number of labels for each class in the dataset:\")\n",
    "        for index, count in label_counts.items():\n",
    "            label = [label for label, idx in self.label_to_index.items() if idx == index][0]\n",
    "            print(f\"Class '{label}': {count} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "data_labels = list(data_processor.folders_with_diseases_labels.values())\n",
    "# 80% training and 20% validation sets\n",
    "training_data, validation_data = train_test_split(data_processor.folder_name_with_diseases, test_size=0.2,\n",
    "                                 random_state=42, shuffle = True, stratify=np.array(data_labels))\n",
    "\n",
    "\n",
    "# CustomDataset for both training and validation\n",
    "train_dataset = CustomDataset(data_processor, folder_names=training_data, transform=transform, is_train=True)\n",
    "train_dataset.print_class_mapping()\n",
    "#train_dataset.print_class_samples()\n",
    "#train_dataset.print_class_labels_count()\n",
    "\n",
    "val_dataset = CustomDataset(data_processor, folder_names=validation_data, transform=transform, is_train=False)\n",
    "#val_dataset.print_class_labels_count()\n",
    "\n",
    "\n",
    "train_labels = []\n",
    "\n",
    "for sample in train_dataset:\n",
    "    _, label = sample\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_label_counts = dict(Counter(train_labels))\n",
    "train_weight_samples = [1/train_label_counts[x] for x in train_labels]\n",
    "\n",
    "train_sampler = WeightedRandomSampler(train_weight_samples, num_samples=len(train_labels), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.5):\n",
    "        super(FineTunedAlexNet, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes= 5\n",
    "model = ImageClassificationModel(num_classes=num_classes).to(device)\n",
    "\n",
    "label_counts = {\n",
    "    'Cardiomegaly': 33,\n",
    "    'Pneumonia': 28,\n",
    "    'Pneumothorax': 15,\n",
    "    'Lung Cancer': 18,\n",
    "    'Effusion': 17,\n",
    "    }\n",
    "\n",
    "# inverse class frequencies\n",
    "total_samples = sum(label_counts.values())\n",
    "class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}\n",
    "\n",
    "# to normalize the weights\n",
    "total_weights = sum(class_weights.values())\n",
    "class_weights = {label: weight / total_weights for label, weight in class_weights.items()}\n",
    "\n",
    "# Convert class_weights to a tensor\n",
    "class_weights_tensor = torch.tensor(list(class_weights.values())).to(device)\n",
    "\n",
    "# weighted CrossEntropyLoss\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.class_weights)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss\n",
    "\n",
    "# Create Focal Loss with class weights\n",
    "criterion = FocalLoss(class_weights_tensor)\n",
    "\n",
    "weight_decay = 5*1e-2\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.03, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0045, weight_decay=weight_decay)\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return accuracy_score(targets.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "num_epochs = 20\n",
    "all_val_predictions = []\n",
    "all_val_targets = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    all_val_targets.append(all_targets)\n",
    "    all_val_predictions.append(all_predictions)\n",
    "\n",
    "    # Save the model if the current validation accuracy is better than the previous best\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'cnn_model.pth')\n",
    "\n",
    "# Calculate the confusion matrix after all epochs\n",
    "all_val_targets = np.concatenate(all_val_targets)\n",
    "all_val_predictions = np.concatenate(all_val_predictions)\n",
    "cm = confusion_matrix(all_val_targets, all_val_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Display confusion matrix using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_counts.keys(), yticklabels=label_counts.keys())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "# save files\n",
    "save_path = '/storage/homefs/zh21i037/'\n",
    "filename = 'confusion matrix.png'\n",
    "save_filename = os.path.join(save_path, filename)\n",
    "\n",
    "plt.savefig(save_filename)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# to save files\n",
    "save_path = '/storage/homefs/zh21i037/'\n",
    "filename = 'losses and accuracies.png'\n",
    "save_filename = os.path.join(save_path, filename)\n",
    "\n",
    "plt.savefig(save_filename)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get probablity distribution of diseases\n",
    "class FineTunedAlexNetClassifier:\n",
    "    def __init__(self, num_classes, model_path):\n",
    "        self.num_classes = num_classes\n",
    "        self.model = FineTunedAlexNet(num_classes)\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "        # preprocess the input image\n",
    "        self.preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def classify_image(self, image_path):\n",
    "        \n",
    "        input_image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.preprocess(input_image)\n",
    "        # add batch dimension to the image\n",
    "        input_batch = input_tensor.unsqueeze(0)  \n",
    "\n",
    "        # predictions\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_batch)\n",
    "\n",
    "        # convert the class output to probability distribution using softmax\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "        # class labels and the corresponding probabilities\n",
    "        class_labels = [f\"Class {i}\" for i in range(self.num_classes)]\n",
    "        class_probabilities = list(zip(class_labels, probabilities))\n",
    "\n",
    "        return class_probabilities\n",
    "\n",
    "\n",
    "model_path = 'cnn_model.pth' \n",
    "image_path = 'Slake1.0/imgs/xmlab333/source.jpg'\n",
    "\n",
    "classifier = FineTunedAlexNetClassifier(num_classes = 5, model_path = model_path)\n",
    "predictions = classifier.classify_image(image_path)\n",
    "\n",
    "print(\"Probability Distribution of Different Classes\")\n",
    "for class_label, probability in predictions:\n",
    "    print(f\"{class_label}: {probability.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
