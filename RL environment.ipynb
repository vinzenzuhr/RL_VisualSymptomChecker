{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e78ae8c-1633-4bff-acf9-39c31127e57f",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "92d17349-7872-4ec0-a307-6787b39324f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import csv \n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os \n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from collections import Counter \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561d990-62ed-4310-84ee-1d654ada0dfb",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "94bafe8f-001e-4d29-aa17-81b3fc6cbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "68c9420f-9f2c-404e-b36d-7b6c0c7fe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, folder_path, disease_list):\n",
    "        self.folder_path = folder_path\n",
    "        self.disease_list = disease_list\n",
    "        self.folders_with_diseases_labels = {}\n",
    "        self.folder_name_with_diseases = []\n",
    "        self.label_counts = None\n",
    "\n",
    "    def read_data(self):\n",
    "        for root, dirs, files in os.walk(os.path.join(self.folder_path, 'imgs')):\n",
    "            for folder_name in dirs:\n",
    "                folder_path = os.path.join(root, folder_name)\n",
    "                \n",
    "                detection_file_path = os.path.join(folder_path, 'detection.json')\n",
    "                with open(detection_file_path, 'r') as detection_file:\n",
    "                    detection_data = json.load(detection_file)\n",
    "\n",
    "                    disease_labels = [label.lower() for item in detection_data for label in item.keys() if label in self.disease_list]\n",
    "                    \n",
    "                    for idx, label in enumerate(disease_labels):\n",
    "                        if label == \"effusion\":\n",
    "                            disease_labels[idx] = \"pleural effusion\"  \n",
    "\n",
    "                    disease_labels = set(disease_labels) \n",
    "                    \n",
    "                    # merge labels for images with multiple labels\n",
    "                    if disease_labels:\n",
    "                        merged_label = '-'.join(sorted(disease_labels))\n",
    "                        self.folders_with_diseases_labels[folder_name] = merged_label\n",
    "                        self.folder_name_with_diseases.append(folder_name)\n",
    "\n",
    "    def delete_folders(self):\n",
    "        # frequency of each merged label\n",
    "        self.label_counts = Counter(self.folders_with_diseases_labels.values())\n",
    "\n",
    "        # delete folders with label counts <= 3\n",
    "        folders_to_delete = [folder_name for folder_name, label in self.folders_with_diseases_labels.items() if self.label_counts[label] <= 3]\n",
    "\n",
    "        for folder_name in folders_to_delete:\n",
    "            del self.folders_with_diseases_labels[folder_name]\n",
    "            self.folder_name_with_diseases.remove(folder_name)\n",
    "            \n",
    "    def get_training_data(self):\n",
    "        training_data = []\n",
    "        for folder_name, label in self.folders_with_diseases_labels.items():\n",
    "            folder_path = os.path.join(self.folder_path, 'imgs', folder_name)\n",
    "            image_path = os.path.join(folder_path, 'source.jpg')\n",
    "            training_data.append((image_path, label))\n",
    "        return training_data\n",
    "\n",
    "\n",
    "folder_path = 'Slake1.0'\n",
    "disease_list = ['Pneumothorax', 'Pneumonia', 'Effusion', 'Lung Cancer']\n",
    "data_processor = DataProcessor(folder_path, disease_list)\n",
    "data_processor.read_data()\n",
    "data_processor.delete_folders()\n",
    "data = data_processor.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "7396cb3a-d3d8-43d1-b089-3672999d21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_CONDITIONS = set(dict(data).values())\n",
    "num_classes=len(SUPPORTED_CONDITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "542de7c8-f8f8-45d9-9fc2-42104e51cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training-test set\n",
    "training_data, validation_data = train_test_split(data, test_size=0.2, random_state=42, shuffle = True, stratify=np.array(data)[:,1])\n",
    "\n",
    "train_labels = [x[1] for x in training_data]\n",
    "train_label_counts = dict(Counter(train_labels))\n",
    "train_weight_samples = [1/train_label_counts[x] for x in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "5bbedf5b-cc6f-4fde-955f-7505aca62eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = WeightedRandomSampler(train_weight_samples, num_samples=len(train_labels), replacement=True)\n",
    "train_dataloader = DataLoader(training_data, sampler=train_sampler, batch_size=2)\n",
    "\n",
    "val_sampler = SequentialSampler(validation_data)\n",
    "val_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b62cb5-7d5e-45b8-a8a2-f38baf682776",
   "metadata": {},
   "source": [
    "### Prepare training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "9f061be0-ee05-4c58-b7df-0c36361cb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FineTunedAlexNet, self).__init__()\n",
    "        \n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "f93bba08-3f21-4ad7-a7a3-34882c366865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Environment \n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class Env: \n",
    "    _condition_symptom_probabilities: Dict[str, Dict[str, float]] # conditions with symptoms and their probabilities\n",
    "    _actions: list[str] # symptoms\n",
    "    _init_state: np.array\n",
    "    _current_state: np.array\n",
    "    _img: Image.Image\n",
    "    _condition: str # the condition which the simulated patient has\n",
    "    _symptoms_of_condition: Dict[str, float] # symptoms of the condition which the simulated patient has\n",
    "    _supported_conditions : list[str]\n",
    "    _cnn_model : FineTunedAlexNet\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img: Image.Image, \n",
    "                 condition: str\n",
    "                ) -> None:  \n",
    "        self._supported_conditions= [\"pneumonia\", \"pneumothorax\", \"lung cancer\", \"pleural effusion\"]\n",
    "        self._img = img\n",
    "        #if(condition is None): \n",
    "        #    condition = random.sample(self._supported_conditions,1)[0]\n",
    "        self._condition = condition\n",
    "\n",
    "        # init condition_symptom_probabilities from HealthKnowledgeGraph.csv\n",
    "        self._condition_symptom_probabilities = self.load_condition_symptom_probabilities()\n",
    "        # init condition_symptom_probabilities from slake knowledge graph \n",
    "        #self._condition_symptom_probabilities= dict()\n",
    "        #with open('Slake1.0/KG/en_disease.csv', newline='') as csvfile:\n",
    "        #    reader = csv.reader(csvfile, delimiter='#')\n",
    "        #    reader.__next__() # skip header \n",
    "        #    for row in reader:\n",
    "        #        if(row[1]!=\"symptom\"):\n",
    "        #            continue\n",
    "        #        if(row[0] not in self._supported_conditions):\n",
    "        #            continue\n",
    "        #        self._condition_symptom_probabilities[row[0]] = dict()\n",
    "        #        n_symptoms=len(row[2].split(','))\n",
    "        #        uniform_prob = 1/(2**n_symptoms)\n",
    "        #        for symptom in row[2].split(','):\n",
    "        #            #assign uniform conditional probability because no conditional probability are available \n",
    "        #            self._condition_symptom_probabilities[row[0]][symptom.strip()] = uniform_prob\n",
    "\n",
    "        # check if condition is valid\n",
    "        if(self._condition not in self._condition_symptom_probabilities.keys()):\n",
    "            raise ValueError('Unknow Condition: ' + condition + '. Please choose one of the following: ' + str(self._condition_symptom_probabilities.keys()))\n",
    "        \n",
    "        # init symptoms_of_condition for easier access\n",
    "        self._symptoms_of_condition = self._condition_symptom_probabilities[self._condition]\n",
    "\n",
    "        # init actions\n",
    "        self._actions = list()\n",
    "        for condition in self._condition_symptom_probabilities.keys(): \n",
    "            for symptom in list(self._condition_symptom_probabilities[condition]):\n",
    "                if symptom not in self._actions:\n",
    "                    self._actions.append(symptom) \n",
    "\n",
    "        #compute visual prior\n",
    "        self._cnn_model = FineTunedAlexNet(num_classes=num_classes).to(device)\n",
    "        #self._cnn_model.load_state_dict(torch.load(\"my_model.pth\"))\n",
    "        self._cnn_model.eval()\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img = transform(self._img).to(device)\n",
    "        visual_prior = self._cnn_model(img[None,:])\n",
    "        print(visual_prior)\n",
    "        # init init_state = vector with cnn output (probabilities per condition) and history of asked symptoms (0=not asked, 1=symptom is present, -1=symptom is not present)\n",
    "        visual_prior = np.random.uniform(size=(len(self._condition_symptom_probabilities.keys()))) #TODO: replace with cnn output\n",
    "        visual_prior = np.ones(shape=(len(self._condition_symptom_probabilities.keys())))\n",
    "        self._init_state = np.concatenate((visual_prior,np.zeros((len(self._actions)))), axis=0)\n",
    "        self._current_state = self._init_state\n",
    "\n",
    "    def load_condition_symptom_probabilities(self) -> Dict[str, Dict[str, float]]:\n",
    "        condition_symptom_probabilities = dict()\n",
    "\n",
    "        with open('HealthKnowledgeGraph.csv', newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            # skip header\n",
    "            header = next(reader)  \n",
    "            for row in reader:\n",
    "                # to make it case insensitive\n",
    "                condition = row[0].lower() \n",
    "                \n",
    "                # if condition is in the supported conditions list then add the symptoms to the list\n",
    "                if condition not in self._supported_conditions:\n",
    "                    continue\n",
    "\n",
    "                symptoms_and_probs = row[1].split(',')\n",
    "                symptom_probabilities = dict()\n",
    "                for symptom_prob in symptoms_and_probs:\n",
    "                    # example for symptom_prob: pain (0.318)\n",
    "                    symptom, prob = map(str.strip, symptom_prob.split('('))\n",
    "                    # to remove the last parentheses ')'\n",
    "                    prob = float(prob[:-1])  \n",
    "                    symptom_probabilities[symptom] = prob\n",
    "                condition_symptom_probabilities[condition] = symptom_probabilities \n",
    "        return condition_symptom_probabilities\n",
    "    \n",
    "    def posterior_of_condition(self, condition: str, useAddition=False) -> float: \n",
    "        #TODO: What is the correct likelihood calculation? If we use multiplication as in P(x,y)=P(x)*P(y), the likelihood gets smaller \n",
    "        #and nothing prevents the model from asking symptoms which are not related to the condition.\n",
    "        if(useAddition):\n",
    "            likelihood=0 \n",
    "        else:\n",
    "            likelihood=1\n",
    "        for idx, symptom in enumerate(self._actions):\n",
    "            patient_answer = self._current_state[idx+len(self._condition_symptom_probabilities.keys())]\n",
    "            #if (patient_answer==1) and (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "            #    likelihood*= 0\n",
    "            #elif (patient_answer==-1) and (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "            #    likelihood*=1\n",
    "            if (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "                #TODO: Do we have to punish the model if a symptom is positive and is not related to the condition?\n",
    "                continue \n",
    "            elif patient_answer==1:\n",
    "                if(useAddition):\n",
    "                    likelihood+=self._condition_symptom_probabilities[condition][symptom]\n",
    "                else:\n",
    "                    likelihood*=self._condition_symptom_probabilities[condition][symptom]\n",
    "            elif patient_answer==-1:\n",
    "                if(useAddition):\n",
    "                    likelihood+=(1-self._condition_symptom_probabilities[condition][symptom]) \n",
    "                else:\n",
    "                    likelihood*=(1-self._condition_symptom_probabilities[condition][symptom]) \n",
    "\n",
    "        prior = self._current_state[list(self._condition_symptom_probabilities.keys()).index(condition)]\n",
    "        if(useAddition):\n",
    "            result = likelihood+prior\n",
    "        else:\n",
    "            result = likelihood*prior\n",
    "        return result\n",
    "    \n",
    "    def reward(self) -> float:\n",
    "        #TODO: Is it a problem when the reward gets smaller and smaller?\n",
    "        #punishment=0\n",
    "        #for i in range(len(self._actions)):\n",
    "        #    patient_answer = self._current_state[i+len(self._condition_symptom_probabilities.keys())]\n",
    "        #    if (patient_answer!=0):\n",
    "        #        punishment+=0.1\n",
    "        return self.posterior_of_condition(self._condition, useAddition=False)#-punishment\n",
    "    \n",
    "    def has_symptom(self, symptom: str) -> bool:\n",
    "        if symptom not in self._symptoms_of_condition:\n",
    "            return False\n",
    "        else:\n",
    "            phi = np.random.uniform()\n",
    "            return phi <= self._symptoms_of_condition[symptom]\n",
    "\n",
    "    def step(self, action_idx: int) -> Transition: \n",
    "        action = self._actions[action_idx]\n",
    "        old_state = self._current_state.copy()\n",
    "        self._current_state[len(self._condition_symptom_probabilities.keys()) + action_idx] = 1 if self.has_symptom(action) else -1\n",
    "\n",
    "        # only give reward if it's a symptom of the condition\n",
    "        if(action in self._symptoms_of_condition):\n",
    "            reward = self.reward()\n",
    "        else:\n",
    "            reward = 0 \n",
    "        return Transition(old_state, action_idx, self._current_state, reward)\n",
    "    \n",
    "    def reset(self) -> np.array:\n",
    "        self._current_state = self._init_state\n",
    "        return self._current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "ac0fbf52-4eed-4213-968f-b60a063075e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory=deque([], maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "c4a72e76-f95a-424e-b4e0-6ae107bb0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128, dtype=torch.double)\n",
    "        self.layer2 = nn.Linear(128, 128, dtype=torch.double)\n",
    "        self.layer3 = nn.Linear(128, n_actions, dtype=torch.double)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "62f58ab8-ba87-4c26-8237-fa4fda48ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[531], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Get number of actions from dummy env \u001b[39;00m\n\u001b[1;32m     18\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSlake1.0/imgs/xmlab333/source.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# TODO: use dummy img\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m myEnv\u001b[38;5;241m=\u001b[39m\u001b[43mEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpneumonia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(myEnv\u001b[38;5;241m.\u001b[39m_actions)\n\u001b[1;32m     21\u001b[0m n_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(myEnv\u001b[38;5;241m.\u001b[39m_current_state)\n",
      "Cell \u001b[0;32mIn[528], line 60\u001b[0m, in \u001b[0;36mEnv.__init__\u001b[0;34m(self, img, condition)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#compute visual prior\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cnn_model \u001b[38;5;241m=\u001b[39m FineTunedAlexNet(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cnn_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cnn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     62\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     63\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[1;32m     64\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     65\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     66\u001b[0m ])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:993\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    991\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    992\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    995\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:447\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled form the replay buffer\n",
    "#GAMMA is the discount factor as mentioned in the previous section\n",
    "#SIZE is the number of transitions sampled from the replay buffer\n",
    "#EPS START is the starting value of epsilon\n",
    "#EPS DECAY controls the rate of exposential decay of epsilon, higher means a slower decay\n",
    "#EPS END is the final value of epsilon\n",
    "#TAU is the update rate of the target network \n",
    "#LR the learning rate of the Adams optimizar\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TAU = 0.005\n",
    "LR = 1e-5\n",
    "\n",
    "#Get number of actions from dummy env \n",
    "img = Image.open(\"Slake1.0/imgs/xmlab333/source.jpg\").convert('RGB') # TODO: use dummy img\n",
    "myEnv=Env(img, 'pneumonia') \n",
    "n_actions = len(myEnv._actions)\n",
    "n_observations = len(myEnv._current_state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b38b8-4bd3-4275-97ba-ee6c519d9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(losses, gradient_norms):\n",
    "    if len(memory) < BATCH_SIZE: \n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    #converts batch_array of Transitions to Transition of batch_arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_state_batch = torch.tensor(batch.next_state)\n",
    "    state_batch = torch.tensor(batch.state)\n",
    "    action_batch = torch.tensor(batch.action)\n",
    "    reward_batch = torch.tensor(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch[None,:])\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(next_state_batch).max(1)[0]\n",
    "\n",
    "    #TODO: state_action_values grow infinitely\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.squeeze(), expected_state_action_values)\n",
    "\n",
    "    losses.append(loss.detach())\n",
    "\n",
    "    parameters = [p for p in policy_net.parameters() if p.grad is not None and p.requires_grad]\n",
    "    if len(parameters) == 0:\n",
    "        total_norm = 0.0\n",
    "    else: \n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()).to(device) for p in parameters]), 2.0).item()\n",
    "    gradient_norms.append(total_norm)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(),100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d30dd9-2f47-40fb-939f-9e1987e782bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(myEnv, state, epsilon):\n",
    "    randnum = np.random.rand(1)\n",
    "    if randnum < epsilon:\n",
    "        action_idx = np.random.randint(len(myEnv._actions))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            action_idx = np.argmax(policy_net(torch.tensor(state))).item() \n",
    "    \n",
    "    return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045de80-5e9f-4510-a6d2-19acf112bd90",
   "metadata": {},
   "source": [
    "### Start with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56974e-27f7-4f4b-a1bf-7204037b1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 400\n",
    "len_episode = 10\n",
    "i_decay=1\n",
    "epsilon = EPS_START\n",
    "losses=[]\n",
    "gradient_norms=[]\n",
    "rewards=[]\n",
    "epsilons=[]\n",
    "total_reward_per_episode=[]\n",
    "\n",
    "for _ in tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:  \n",
    "        condition = batch[1][0]\n",
    "        image_path = batch[0][0]\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        myEnv=Env(img, condition)\n",
    "        state = myEnv.reset()\n",
    "        for _ in range(len_episode):\n",
    "            if epsilon > EPS_END:\n",
    "                epsilon = EPS_START * math.exp(-i_decay/EPS_DECAY)\n",
    "                i_decay+=1 \n",
    "            epsilons.append(epsilon)\n",
    "            action_idx = select_action(myEnv, state, epsilon)\n",
    "            transition = myEnv.step(action_idx)\n",
    "            rewards.append(transition.reward)\n",
    "            last_reward=transition.reward\n",
    "            memory.push(transition)\n",
    "            state = transition.next_state\n",
    "            optimize_model(losses, gradient_norms)\n",
    "    \n",
    "            #Soft update of target  network weights\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "        total_reward_per_episode.append(last_reward) \n",
    "\n",
    "print(\"complete\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a918b5-ab09-4d42-9df5-12b24443939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def averagewindow(R, d=1):\n",
    "    n = len(R)\n",
    "    t = []\n",
    "    y = []\n",
    "    for i in range(0,int(n/d)):\n",
    "        t.append(np.mean(range(i*d,(i+1)*d)))\n",
    "        y.append(np.mean(R[i*d:min(n,(i+1)*d)]))\n",
    "    return t,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cc223-d43b-46c7-a64b-5b3b31e04d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bd178-8674-4f0c-8bcd-e5d475ad2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y = averagewindow(losses, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceae2b1-ad50-42c7-adaa-81fd2997f226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t,y = averagewindow(gradient_norms, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"gradient norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29b034-6baa-4840-ba88-5bbbcaa469de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y = averagewindow(rewards, d=800)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb4006-6ae5-4344-a24f-455bea39f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y = averagewindow(total_reward_per_episode, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"Total reward per episode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0d4c9-56e8-43f4-9620-cc0f6477713e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71ef87-1b20-4573-b541-0a90c1ba4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topKAccuracy(k=3):\n",
    "    epsilon = 0\n",
    "    N_samples=0\n",
    "    N_correct_samples=0\n",
    "    for sample in test:\n",
    "        N_samples+=1\n",
    "        sample_condition = \"\" # include condition\n",
    "        myEnv=Env(np.array([]), condition=sample_condition) # TODO: include picture\n",
    "        state = myEnv.reset()\n",
    "        for _ in range(len_episodes):\n",
    "            action_idx = select_action(myEnv, state, epsilon)\n",
    "            transition = myEnv.step(action_idx)\n",
    "            state = transition.next_state\n",
    "        posterior_of_conditions = []\n",
    "        for condition in myEnv._supported_conditions:\n",
    "            posterior = myEnv.posterior_of_condition(condition, useAddition=False)\n",
    "            posterior_of_conditions.append((posterior, condition))\n",
    "        posterior_of_conditions.sort(key=lambda x: x[0]) \n",
    "        sample_k = next(x for x, val in enumerate(posterior_of_conditions)\n",
    "                                  if val == sample_condition)\n",
    "        if(sample_k <= k):\n",
    "            N_correct_samples+=1\n",
    "    return N_correct_samples / N_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2cacf-7e24-42cd-944e-67ad9c4fde4f",
   "metadata": {},
   "source": [
    "### Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing simulated patient answers\n",
    "myTestEnv=Env(np.array([]), 'pneumothorax')\n",
    "print(\"Symptoms for Pertussis:\")\n",
    "print(myTestEnv._condition_symptom_probabilities['pneumothorax'])\n",
    "print(\"Expected uniform conditional proabability: 1\\(\", 2**len(myTestEnv._condition_symptom_probabilities['pneumothorax'].keys()), \")\")\n",
    "n=0\n",
    "prob=0\n",
    "for i in range(10000):\n",
    "    n+=1\n",
    "    if myTestEnv.has_symptom('pain'):\n",
    "        prob+=1 \n",
    "print(\"\\n Probability of spastic cough after 10000 samples: \" + str(prob/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae050ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing reward\n",
    "myTestEnv=Env(np.array([]), 'pneumothorax')\n",
    "print(\"prior of condition:\")\n",
    "print(myTestEnv._current_state[list(myTestEnv._condition_symptom_probabilities.keys()).index(\"pneumothorax\")])\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('coughing'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('coughing')] \n",
    "print(\"Probability of coughing: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['coughing']))\n",
    "print(\"Result patient asking if he has coughing: \" + str(result))\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('pain'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('pain')] \n",
    "print(\"Probability of pain: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['pain']))\n",
    "print(\"Result patient asking if he has pain: \" + str(result))\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('shortness of breath'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('shortness of breath')] \n",
    "print(\"Probability of shortness of breath: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['shortness of breath']))\n",
    "print(\"Result patient asking if he has shortness of breath: \" + str(result))\n",
    "\n",
    "print(\"Reward: \" + str(myTestEnv.reward()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0acb9-cfc8-4608-898e-1921392716df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
