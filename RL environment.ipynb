{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e78ae8c-1633-4bff-acf9-39c31127e57f",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "92d17349-7872-4ec0-a307-6787b39324f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import csv \n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os \n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from collections import Counter \n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "import time\n",
    "import concurrent.futures\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561d990-62ed-4310-84ee-1d654ada0dfb",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "94bafe8f-001e-4d29-aa17-81b3fc6cbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "68c9420f-9f2c-404e-b36d-7b6c0c7fe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, folder_path, disease_list):\n",
    "        self.folder_path = folder_path\n",
    "        self.disease_list = disease_list\n",
    "        self.folders_with_diseases_labels = {}\n",
    "        self.folder_name_with_diseases = []\n",
    "        self.label_counts = None\n",
    "\n",
    "    def read_data(self):\n",
    "        for root, dirs, files in os.walk(os.path.join(self.folder_path, 'imgs')):\n",
    "            for folder_name in dirs:\n",
    "                folder_path = os.path.join(root, folder_name)\n",
    "                \n",
    "                detection_file_path = os.path.join(folder_path, 'detection.json')\n",
    "                with open(detection_file_path, 'r') as detection_file:\n",
    "                    detection_data = json.load(detection_file)\n",
    "\n",
    "                    disease_labels = [label.lower() for item in detection_data for label in item.keys() if label in self.disease_list]\n",
    "                    \n",
    "                    for idx, label in enumerate(disease_labels):\n",
    "                        if label == \"effusion\":\n",
    "                            disease_labels[idx] = \"pleural effusion\" \n",
    "                        elif label == \"cardiomegaly\":\n",
    "                            disease_labels[idx] = \"cardiomyopathy\"\n",
    "                             \n",
    "                    disease_labels = set(disease_labels) \n",
    "                    \n",
    "                    # merge labels for images with multiple labels\n",
    "                    if disease_labels:\n",
    "                        merged_label = '-'.join(sorted(disease_labels))\n",
    "                        self.folders_with_diseases_labels[folder_name] = merged_label\n",
    "                        self.folder_name_with_diseases.append(folder_name)\n",
    "\n",
    "    def delete_folders(self):\n",
    "        # frequency of each merged label\n",
    "        self.label_counts = Counter(self.folders_with_diseases_labels.values())\n",
    "\n",
    "        # delete folders with label counts <= 3\n",
    "        folders_to_delete = [folder_name for folder_name, label in self.folders_with_diseases_labels.items() if self.label_counts[label] <= 3]\n",
    "\n",
    "        for folder_name in folders_to_delete:\n",
    "            del self.folders_with_diseases_labels[folder_name]\n",
    "            self.folder_name_with_diseases.remove(folder_name)\n",
    "            \n",
    "    def get_training_data(self):\n",
    "        training_data = []\n",
    "        for folder_name, label in self.folders_with_diseases_labels.items():\n",
    "            folder_path = os.path.join(self.folder_path, 'imgs', folder_name)\n",
    "            image_path = os.path.join(folder_path, 'source.jpg') \n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            img = transform(img)\n",
    "            training_data.append((img, label))\n",
    "        return training_data\n",
    "\n",
    "\n",
    "folder_path = 'Slake1.0'\n",
    "disease_list = ['Pneumothorax', 'Pneumonia', 'Effusion', 'Lung Cancer', \"Cardiomegaly\"]\n",
    "data_processor = DataProcessor(folder_path, disease_list)\n",
    "data_processor.read_data()\n",
    "data_processor.delete_folders()\n",
    "data = data_processor.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7396cb3a-d3d8-43d1-b089-3672999d21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels=[x[1] for x in data]\n",
    "SUPPORTED_CONDITIONS = set(data_labels) \n",
    "num_classes=len(SUPPORTED_CONDITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "542de7c8-f8f8-45d9-9fc2-42104e51cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training-test set\n",
    "training_data, validation_data = train_test_split(data, test_size=0.2, random_state=42, shuffle = True, stratify=np.array(data_labels))\n",
    "\n",
    "train_labels = [x[1] for x in training_data]\n",
    "train_label_counts = dict(Counter(train_labels))\n",
    "train_weight_samples = [1/train_label_counts[x] for x in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5bbedf5b-cc6f-4fde-955f-7505aca62eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = WeightedRandomSampler(train_weight_samples, num_samples=len(train_labels), replacement=True)\n",
    "train_dataloader = DataLoader(training_data, sampler=train_sampler, batch_size=1)\n",
    "\n",
    "val_sampler = SequentialSampler(validation_data)\n",
    "val_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b62cb5-7d5e-45b8-a8a2-f38baf682776",
   "metadata": {},
   "source": [
    "### Prepare training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9f061be0-ee05-4c58-b7df-0c36361cb1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FineTunedAlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=4096, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN Model\n",
    "class FineTunedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FineTunedAlexNet, self).__init__()\n",
    "        \n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = FineTunedAlexNet(num_classes=num_classes).to(device)\n",
    "cnn_model.load_state_dict(torch.load(\"cnn_model.pth\", map_location=torch.device(device)))\n",
    "cnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f93bba08-3f21-4ad7-a7a3-34882c366865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Environment \n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class Env: \n",
    "    _condition_symptom_probabilities: Dict[str, Dict[str, float]] # conditions with symptoms and their probabilities\n",
    "    _actions: list[str] # symptoms\n",
    "    _init_state: np.array\n",
    "    _current_state: np.array\n",
    "    _img: torch.tensor\n",
    "    _condition: str # the condition which the simulated patient has\n",
    "    _symptoms_of_condition: Dict[str, float] # symptoms of the condition which the simulated patient has\n",
    "    _supported_conditions: list[str]\n",
    "    _cnn_model: FineTunedAlexNet\n",
    "    _num_asked_symptoms: int\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img: torch.tensor, \n",
    "                 condition: str,\n",
    "                 cnn_model: FineTunedAlexNet\n",
    "                ) -> None:  \n",
    "        self._supported_conditions= [\"pneumonia\", \"pneumothorax\", \"lung cancer\", \"pleural effusion\", \"cardiomyopathy\"]\n",
    "        self._img = img\n",
    "        self._cnn_model = cnn_model \n",
    "        self._num_asked_symptoms = 0\n",
    "        self._episode_length = 10\n",
    "        #if(condition is None): \n",
    "        #    condition = random.sample(self._supported_conditions,1)[0]\n",
    "        self._condition = condition\n",
    "\n",
    "        # init condition_symptom_probabilities from HealthKnowledgeGraph.csv\n",
    "        self._condition_symptom_probabilities = self.load_condition_symptom_probabilities()\n",
    "        # init condition_symptom_probabilities from slake knowledge graph \n",
    "        #self._condition_symptom_probabilities= dict()\n",
    "        #with open('Slake1.0/KG/en_disease.csv', newline='') as csvfile:\n",
    "        #    reader = csv.reader(csvfile, delimiter='#')\n",
    "        #    reader.__next__() # skip header \n",
    "        #    for row in reader:\n",
    "        #        if(row[1]!=\"symptom\"):\n",
    "        #            continue\n",
    "        #        if(row[0] not in self._supported_conditions):\n",
    "        #            continue\n",
    "        #        self._condition_symptom_probabilities[row[0]] = dict()\n",
    "        #        n_symptoms=len(row[2].split(','))\n",
    "        #        uniform_prob = 1/(2**n_symptoms)\n",
    "        #        for symptom in row[2].split(','):\n",
    "        #            #assign uniform conditional probability because no conditional probability are available \n",
    "        #            self._condition_symptom_probabilities[row[0]][symptom.strip()] = uniform_prob\n",
    "\n",
    "        # check if condition is valid\n",
    "        if(self._condition not in self._condition_symptom_probabilities.keys()):\n",
    "            raise ValueError('Unknow Condition: ' + condition + '. Please choose one of the following: ' + str(self._condition_symptom_probabilities.keys()))\n",
    "        \n",
    "        # init symptoms_of_condition for easier access\n",
    "        self._symptoms_of_condition = self._condition_symptom_probabilities[self._condition]\n",
    "\n",
    "        # init actions\n",
    "        self._actions = list()\n",
    "        for condition in self._condition_symptom_probabilities.keys(): \n",
    "            for symptom in list(self._condition_symptom_probabilities[condition]):\n",
    "                if symptom not in self._actions:\n",
    "                    self._actions.append(symptom) \n",
    "\n",
    "\n",
    "        \n",
    "        #compute visual prior\n",
    "        logits = self._cnn_model(img[None,:].to(device))[0]\n",
    "        logits = logits.cpu()\n",
    "        #sort logits to the same order as in self._condition_symptom_probabilities. CNN_model output: Lung Cancer: idx 0, Pneumothorax: idx 1, Pneumonia: idx 2, Effusion: idx 3, Cardiomegaly: idx 4 \n",
    "        #logit_indicies = {\n",
    "        #    \"lung cancer\": 0,\n",
    "        #    \"pneumothorax\": 1, \n",
    "        #    \"pneumonia\": 2, \n",
    "        #    \"pleural effusion\": 3, \n",
    "        #    \"cardiomyopathy\": 4\n",
    "        #}  \n",
    "        logit_indicies = {\n",
    "            \"cardiomyopathy\": 0,\n",
    "            \"pneumothorax\": 1,\n",
    "            \"pneumonia\": 2, \n",
    "            \"lung cancer\": 3,\n",
    "            \"pleural effusion\": 4, \n",
    "        }\n",
    "        condition_logit_idx = [logit_indicies[c] for c in self._condition_symptom_probabilities.keys()]\n",
    "        visual_prior = softmax(torch.tensor([logits[idx] for idx in condition_logit_idx]))\n",
    "        \n",
    "        #visual_prior = np.ones(shape=(len(self._condition_symptom_probabilities.keys()))) #TODO: replace with cnn output \n",
    "        # init init_state = vector with cnn output (probabilities per condition) and history of asked symptoms (0=not asked, 1=symptom is present, -1=symptom is not present)\n",
    "        self._init_state = np.concatenate((visual_prior,np.zeros((len(self._actions)))), axis=0)\n",
    "        self._current_state = self._init_state\n",
    "\n",
    "    def load_condition_symptom_probabilities(self) -> Dict[str, Dict[str, float]]:\n",
    "        condition_symptom_probabilities = dict()\n",
    "\n",
    "        with open('HealthKnowledgeGraph.csv', newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            # skip header\n",
    "            header = next(reader)  \n",
    "            for row in reader:\n",
    "                # to make it case insensitive\n",
    "                condition = row[0].lower() \n",
    "                \n",
    "                # if condition is in the supported conditions list then add the symptoms to the list\n",
    "                if condition not in self._supported_conditions:\n",
    "                    continue\n",
    "\n",
    "                symptoms_and_probs = row[1].split(',')\n",
    "                symptom_probabilities = dict()\n",
    "                for symptom_prob in symptoms_and_probs:\n",
    "                    # example for symptom_prob: pain (0.318)\n",
    "                    symptom, prob = map(str.strip, symptom_prob.split('('))\n",
    "                    # to remove the last parentheses ')'\n",
    "                    prob = float(prob[:-1])  \n",
    "                    symptom_probabilities[symptom] = prob\n",
    "                condition_symptom_probabilities[condition] = symptom_probabilities \n",
    "        return condition_symptom_probabilities\n",
    "\n",
    "    def final_state(self) -> bool:\n",
    "        if(self._num_asked_symptoms >= self._episode_length):\n",
    "            return True\n",
    "        else:\n",
    "            return False             \n",
    "    \n",
    "    def posterior_of_condition(self, condition: str, useAddition=False) -> float: \n",
    "        #TODO: What is the correct likelihood calculation? If we use multiplication as in P(x,y)=P(x)*P(y), the likelihood gets smaller \n",
    "        #and nothing prevents the model from asking symptoms which are not related to the condition.\n",
    "        if(useAddition):\n",
    "            likelihood=0\n",
    "        else:\n",
    "            likelihood=1\n",
    "        for idx, symptom in enumerate(self._actions):\n",
    "            patient_answer = self._current_state[idx+len(self._condition_symptom_probabilities.keys())]\n",
    "            #if (patient_answer==1) and (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "            #    likelihood*= 0\n",
    "            #elif (patient_answer==-1) and (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "            #    likelihood*=1\n",
    "            if (symptom not in self._condition_symptom_probabilities[condition].keys()):\n",
    "                #TODO: Do we have to punish the model if a symptom is positive and is not related to the condition?\n",
    "                continue \n",
    "            elif patient_answer==1:\n",
    "                if(useAddition):\n",
    "                    likelihood+=self._condition_symptom_probabilities[condition][symptom]\n",
    "                else:\n",
    "                    likelihood*=self._condition_symptom_probabilities[condition][symptom]\n",
    "            elif patient_answer==-1:\n",
    "                if(useAddition):\n",
    "                    likelihood+=(1-self._condition_symptom_probabilities[condition][symptom]) \n",
    "                else:\n",
    "                    likelihood*=(1-self._condition_symptom_probabilities[condition][symptom]) \n",
    "\n",
    "        prior = self._current_state[list(self._condition_symptom_probabilities.keys()).index(condition)]\n",
    "        if(useAddition):\n",
    "            result = likelihood+prior\n",
    "        else:\n",
    "            result = likelihood*prior\n",
    "        return result\n",
    "    \n",
    "    def reward(self) -> float:\n",
    "        #TODO: Is it a problem when the reward gets smaller and smaller?\n",
    "        punishment=0\n",
    "        for i in range(len(self._actions)):\n",
    "            patient_answer = self._current_state[i+len(self._condition_symptom_probabilities.keys())]\n",
    "            if (patient_answer!=0):\n",
    "                punishment+=0.3\n",
    "        return self.posterior_of_condition(self._condition, useAddition=True)-punishment\n",
    "    \n",
    "    def has_symptom(self, symptom: str) -> bool:\n",
    "        if symptom not in self._symptoms_of_condition:\n",
    "            return False\n",
    "        else:\n",
    "            phi = np.random.uniform()\n",
    "            return phi <= self._symptoms_of_condition[symptom]\n",
    "\n",
    "    def step(self, action_idx: int) -> Transition:\n",
    "        \n",
    "        if(self.final_state()):\n",
    "            return Transition(self._current_state, action_idx, None, 0)\n",
    "        \n",
    "        action = self._actions[action_idx]\n",
    "        old_state = self._current_state.copy()\n",
    "        self._current_state[len(self._condition_symptom_probabilities.keys()) + action_idx] = 1 if self.has_symptom(action) else -1\n",
    "\n",
    "        self._num_asked_symptoms+=1\n",
    "\n",
    "        # only give reward if it's a symptom of the condition\n",
    "        #if(action in self._symptoms_of_condition):\n",
    "        #    reward = self.reward()\n",
    "        #else:\n",
    "        #    reward = 0 \n",
    "        return Transition(old_state, action_idx, self._current_state, self.reward())\n",
    "    \n",
    "    def reset(self) -> np.array:\n",
    "        self._current_state = self._init_state\n",
    "        return self._current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ac0fbf52-4eed-4213-968f-b60a063075e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay for RL\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory=deque([], maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c4a72e76-f95a-424e-b4e0-6ae107bb0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128, dtype=torch.double)\n",
    "        self.layer2 = nn.Linear(128, 128, dtype=torch.double)\n",
    "        self.layer3 = nn.Linear(128, n_actions, dtype=torch.double)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "62f58ab8-ba87-4c26-8237-fa4fda48ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255/1532341961.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  visual_prior = softmax(torch.tensor([logits[idx] for idx in condition_logit_idx]))\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled form the replay buffer\n",
    "#GAMMA is the discount factor as mentioned in the previous section\n",
    "#SIZE is the number of transitions sampled from the replay buffer\n",
    "#EPS START is the starting value of epsilon\n",
    "#EPS DECAY controls the rate of exposential decay of epsilon, higher means a slower decay\n",
    "#EPS END is the final value of epsilon\n",
    "#TAU is the update rate of the target network \n",
    "#LR the learning rate of the Adams optimizar\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100#2000 #with 2000 epsilon is after 8000 steps at 0.05\n",
    "TAU = 0.005\n",
    "LR = 1e-5\n",
    "\n",
    "#Get number of actions from dummy env \n",
    "img = Image.open(\"Slake1.0/imgs/xmlab333/source.jpg\").convert('RGB') # TODO: use dummy img\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "img = transform(img) \n",
    "myEnv=Env(img, 'pneumonia', cnn_model) \n",
    "n_actions = len(myEnv._actions)\n",
    "n_observations = len(myEnv._current_state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "562b38b8-4bd3-4275-97ba-ee6c519d9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(losses, gradient_norms):\n",
    "    policy_net.train()\n",
    "    if len(memory) < BATCH_SIZE: \n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE) \n",
    "    #converts batch_array of Transitions to Transition of batch_arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([torch.tensor(s) for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.tensor(batch.state).to(device)\n",
    "    action_batch = torch.tensor(batch.action).to(device)\n",
    "    reward_batch = torch.tensor(batch.reward).to(device)\n",
    "\n",
    "    pred=policy_net(state_batch)\n",
    "    state_action_values = pred[torch.arange(pred.shape[0]), action_batch]\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device, dtype=torch.double)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    #TODO: state_action_values grow infinitely\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    #print(\"state_action_values: \", state_action_values)\n",
    "    #print(\"action_batch: \", action_batch)\n",
    "    #print(\"reward batch: \", reward_batch)\n",
    "    #print(\"state batch[0]: \", state_batch[0]) \n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    losses.append((loss.detach()).cpu())\n",
    "\n",
    "    parameters = [p for p in policy_net.parameters() if p.grad is not None and p.requires_grad]\n",
    "    if len(parameters) == 0:\n",
    "        total_norm = 0.0\n",
    "    else: \n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach()).cpu() for p in parameters]), 2.0).item()\n",
    "    gradient_norms.append(total_norm)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(),100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e2d30dd9-2f47-40fb-939f-9e1987e782bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(myEnv, state, epsilon):\n",
    "    randnum = np.random.rand(1) \n",
    "    if randnum < epsilon:\n",
    "        action_idx = np.random.randint(len(myEnv._actions))\n",
    "        #print(\"random\")\n",
    "    else:  \n",
    "        with torch.no_grad():\n",
    "            action_idx = np.argmax(policy_net(torch.tensor(state).to(device)).cpu()).item() \n",
    "            #print(\"non random\")\n",
    "            #print(\"network: \", policy_net(torch.tensor(state).to(device)).cpu())\n",
    "    \n",
    "    return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045de80-5e9f-4510-a6d2-19acf112bd90",
   "metadata": {},
   "source": [
    "### Start with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0659a351-ee8a-4db0-bb8f-7b9bb0a949c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicies = {\n",
    "#            \"cardiomyopathy\": 0, # 0 TP\n",
    "#            \"pneumothorax\": 1,\n",
    "#            \"pneumonia\": 2, \n",
    "#            \"lung cancer\": 3,\n",
    "#            \"pleural effusion\": 4, # 0 TP\n",
    "#        }#\n",
    "\n",
    "#N=0\n",
    "#TP=0 \n",
    "#for batch in train_dataloader:\n",
    "#    condition = batch[1][0]\n",
    "#    img = batch[0][0]\n",
    "#    \n",
    "#    #if(condition!=\"pleural effusion\"):\n",
    "#    #    continue\n",
    "#    logits=cnn_model(img[None,:].to(device))\n",
    "#    probs = softmax(logits.detach())\n",
    "#    #print(probs)\n",
    "#    #myEnv=Env(img, condition, cnn_model)\n",
    "#    #state = myEnv.reset() \n",
    "#    N+=1 \n",
    "#    if(np.argmax(probs) == indicies[condition]):\n",
    "#        TP+=1\n",
    "#print(N)\n",
    "#print(TP)\n",
    "#print(TP/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "501218df-297f-49cb-9aea-7c9eec1d0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = Lock() \n",
    "def training_episode(img: torch.tensor, condition: str, epsilon: float):\n",
    "    myEnv=Env(img, condition, cnn_model)\n",
    "    state = myEnv.reset()\n",
    "    #print(\"new condition: \", condition) \n",
    "    for _ in range(len_episode):  \n",
    "        action_idx = select_action(myEnv, state, epsilon)\n",
    "        #print(action_idx)\n",
    "        transition = myEnv.step(action_idx)\n",
    "        last_reward=transition.reward\n",
    "        state = transition.next_state\n",
    "        #print(transition)\n",
    "        with lock: \n",
    "            rewards.append(transition.reward)\n",
    "            memory.push(transition) \n",
    "    with lock:\n",
    "        for _ in range(len_episode):  \n",
    "            optimize_model(losses, gradient_norms)\n",
    "    \n",
    "            #Soft update of target network weights\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict) \n",
    "        total_reward_per_episode.append(last_reward) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "03c9cabb-2c60-44b8-8b73-3fca2173ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1300#1600#2500\n",
    "len_episode = 11\n",
    "i_decay=1\n",
    "epsilon = EPS_START\n",
    "losses=[]\n",
    "gradient_norms=[]\n",
    "rewards=[]\n",
    "epsilons=[]\n",
    "total_reward_per_episode=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56974e-27f7-4f4b-a1bf-7204037b1352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1300 [00:00<?, ?it/s]/tmp/ipykernel_255/1532341961.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  visual_prior = softmax(torch.tensor([logits[idx] for idx in condition_logit_idx]))\n",
      "  0%|▏                                                                              | 4/1300 [02:21<16:16:21, 45.20s/it]"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:  \n",
    "        if epsilon > EPS_END:\n",
    "            epsilon = EPS_START * math.exp(-i_decay/EPS_DECAY)\n",
    "            i_decay+=1 \n",
    "        epsilons.append(epsilon)\n",
    "        condition = batch[1][0]\n",
    "        img = batch[0][0] \n",
    "        training_episode(img, condition, epsilon)\n",
    "        #pool.map(training_episode, (img, condition, epsilon))\n",
    "pool.shutdown(wait=True)\n",
    "print(\"complete\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bb421-b7da-4ea9-9dae-b451a6270a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'RL_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a918b5-ab09-4d42-9df5-12b24443939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def averagewindow(R, d=1):\n",
    "    n = len(R)\n",
    "    t = []\n",
    "    y = []\n",
    "    for i in range(0,int(n/d)):\n",
    "        t.append(np.mean(range(i*d,(i+1)*d)))\n",
    "        y.append(np.mean(R[i*d:min(n,(i+1)*d)]))\n",
    "    return t,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cc223-d43b-46c7-a64b-5b3b31e04d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilons)\n",
    "plt.savefig('epsilons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bd178-8674-4f0c-8bcd-e5d475ad2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y = averagewindow(losses, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"losses\")\n",
    "plt.savefig('losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceae2b1-ad50-42c7-adaa-81fd2997f226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t,y = averagewindow(gradient_norms, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"gradient norms\")\n",
    "plt.savefig('gradient norms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29b034-6baa-4840-ba88-5bbbcaa469de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y = averagewindow(rewards, d=800)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"Rewards\")\n",
    "plt.savefig('rewards.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb4006-6ae5-4344-a24f-455bea39f367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t,y = averagewindow(total_reward_per_episode, d=50)\n",
    "plt.plot(t,y)\n",
    "plt.title(\"Total reward per episode\")\n",
    "plt.savefig('total reward per episode.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0d4c9-56e8-43f4-9620-cc0f6477713e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71ef87-1b20-4573-b541-0a90c1ba4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topKAccuracy(k=3):\n",
    "    policy_net.eval()\n",
    "    epsilon = 0\n",
    "    N_samples=0\n",
    "    N_correct_samples=0\n",
    "    for batch in val_dataloader:\n",
    "        N_samples+=1\n",
    "        condition = batch[1][0]\n",
    "        img = batch[0][0] \n",
    "        print(\"Condition: \", condition)\n",
    "        myEnv=Env(img, condition, cnn_model) \n",
    "        state = myEnv.reset()\n",
    "        # ask patient 10 symptoms\n",
    "        for _ in range(len_episode):\n",
    "            action_idx = select_action(myEnv, state, epsilon)\n",
    "            print(\"Action: \", myEnv._actions[action_idx])\n",
    "            transition = myEnv.step(action_idx)\n",
    "            print(\"Reward: \", transition.reward)\n",
    "            state = transition.next_state\n",
    "        #calculate posterior for every conditions\n",
    "        posterior_of_conditions = []\n",
    "        for condition in myEnv._supported_conditions:\n",
    "            posterior = myEnv.posterior_of_condition(condition, useAddition=False)\n",
    "            #set posterior to 0 if no symptom is related to the condition and therefore the likelihood stays\n",
    "            #TODO: Delete when cnn is integrated\n",
    "            if(posterior==1):\n",
    "                posterior=0\n",
    "            posterior_of_conditions.append((posterior, condition))\n",
    "        #sort posteriors by value\n",
    "        posterior_of_conditions.sort(key=lambda x: x[0])\n",
    "        #get rank of posterior of correct condition\n",
    "        rank = 1+next(i for i, val in enumerate(posterior_of_conditions)\n",
    "                                  if val[1] == condition)\n",
    "        if(rank <= k):\n",
    "            N_correct_samples+=1 \n",
    "        print(posterior_of_conditions) \n",
    "    return N_correct_samples / N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89719c-6760-42c8-9077-454b6dd7078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topKAccuracy(k=3)) #Random model would have 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2cacf-7e24-42cd-944e-67ad9c4fde4f",
   "metadata": {},
   "source": [
    "### Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbce86f-8676-4062-858f-90d4d33c9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing simulated patient answers\n",
    "myTestEnv=Env(np.array([]), 'pneumothorax', cnn_model)\n",
    "print(\"Symptoms for Pertussis:\")\n",
    "print(myTestEnv._condition_symptom_probabilities['pneumothorax'])\n",
    "print(\"Expected uniform conditional proabability: 1\\(\", 2**len(myTestEnv._condition_symptom_probabilities['pneumothorax'].keys()), \")\")\n",
    "n=0\n",
    "prob=0\n",
    "for i in range(10000):\n",
    "    n+=1\n",
    "    if myTestEnv.has_symptom('pain'):\n",
    "        prob+=1 \n",
    "print(\"\\n Probability of spastic cough after 10000 samples: \" + str(prob/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae050ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing reward\n",
    "myTestEnv=Env(np.array([]), 'pneumothorax', cnn_model)\n",
    "print(\"prior of condition:\")\n",
    "print(myTestEnv._current_state[list(myTestEnv._condition_symptom_probabilities.keys()).index(\"pneumothorax\")])\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('coughing'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('coughing')] \n",
    "print(\"Probability of coughing: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['coughing']))\n",
    "print(\"Result patient asking if he has coughing: \" + str(result))\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('pain'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('pain')] \n",
    "print(\"Probability of pain: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['pain']))\n",
    "print(\"Result patient asking if he has pain: \" + str(result))\n",
    "\n",
    "myTestEnv.step(myTestEnv._actions.index('shortness of breath'))\n",
    "result=myTestEnv._current_state[len(myTestEnv._condition_symptom_probabilities.keys()) + list(myTestEnv._actions).index('shortness of breath')] \n",
    "print(\"Probability of shortness of breath: \" + str(myTestEnv._condition_symptom_probabilities['pneumothorax']['shortness of breath']))\n",
    "print(\"Result patient asking if he has shortness of breath: \" + str(result))\n",
    "\n",
    "print(\"Reward: \" + str(myTestEnv.reward()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0acb9-cfc8-4608-898e-1921392716df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create python script for ubelix\n",
    "#!jupyter nbconvert --to script \"RL environment.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
